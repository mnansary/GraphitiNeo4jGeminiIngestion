# ===================================================================
# Main Configuration for the Graphiti Application
# This file serves as the single source of truth for all services.
# ===================================================================

# -------------------------------------------------------------------
# Neo4j Database Connection
# Configuration for connecting to the Neo4j graph database.
# As per your docker command, auth is disabled.
# -------------------------------------------------------------------
neo4j:
  uri: "bolt://localhost:7687"
  # For NEO4J_AUTH=none, user and password can be empty.
  user: ""
  password: ""

# -------------------------------------------------------------------
# Gemini API Manager Configuration
# Settings for the manager that handles API key rotation and rate limiting.
# -------------------------------------------------------------------
gemini_api_manager:
  # Path to the CSV file containing your Google API keys under an 'api' header.
  api_key_csv_path: "configs/google_apis.csv"
  # Path to the YAML file defining model capabilities and task mappings.
  model_config_path: "configs/model.yaml"
  # Connection details for the Redis instance used for distributed state.
  redis_host: "localhost"
  redis_port: 6379

# -------------------------------------------------------------------
# Jina V3 Triton Embedder Configuration
# Settings for the custom embedder client that connects to your Triton server.
# -------------------------------------------------------------------
jina_triton_embedder:
  # The base URL of your running Triton Inference Server.
  triton_url: "http://localhost:8000"
  # The name of the model in Triton used for embedding search queries.
  query_model_name: "jina_query"
  # The name of the model in Triton used for embedding documents/passages.
  passage_model_name: "jina_passage"
  # The name of the Hugging Face tokenizer required by the Jina model.
  tokenizer_name: "jinaai/jina-embeddings-v3"
  # The name of the output tensor from the Triton model config.pbtxt.
  triton_output_name: "text_embeds"
  # The number of texts to process in a single batch request to Triton.
  batch_size: 1

# -------------------------------------------------------------------
# Managed Gemini Client (LLM) Configuration
# Settings for the custom graphiti-core LLM client.
# -------------------------------------------------------------------
managed_gemini_client:
  # Default temperature for LLM text generation.
  temperature: 0.3
  # Default model size to request from the manager. Can be 'medium' or 'small'.
  default_model_size: "medium"

# -------------------------------------------------------------------
# Managed Gemini Reranker (Cross-Encoder) Configuration
# -------------------------------------------------------------------
managed_gemini_reranker:
  # You can specify a preferred model for reranking here.
  # The manager will still fall back to other TEXT_TO_TEXT models if this one is busy.
  # A fast, cheap model is ideal for this task.
  model: "gemini-2.5-flash-lite"


# -------------------------------------------------------------------
# Logging Configuration
# Settings for the Loguru logger setup.
# -------------------------------------------------------------------
logging:
  # Log level for console output (DEBUG, INFO, WARNING, ERROR).
  console_level: "INFO"
  # Log level for file output. DEBUG is recommended for files.
  file_level: "DEBUG"
  # Directory where log files will be stored.
  log_dir: "logs"
  # How long to keep old log files (e.g., "10 days", "1 month").
  retention: "10 days"
  # Condition for creating a new log file (e.g., "10 MB", "1 week", "12:00").
  rotation: "10 MB"